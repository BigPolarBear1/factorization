DISCLAIMER: AT NO POINT IN MY 2.5 YEARS OF RESEARCH DID I USE AI OR HAS AI IN ANY WAY HELPED WITH MY RESEARCH. I'M PUTTING THIS HERE BECAUSE I KNOW PEOPLE WILL ACCUSE ME OF IT. 

Note: Remember, as this now comes to completion in the coming days. Both the FBI and Microsoft knew I was working on factorization when I had only been working on it for a few months and nobody else in the entire world knew I was working on it. I tried desperately to avoid the path of public disclosure as I would have much rather sold my work. But at every turn people made it impossible. This would not have happened 10 years ago. An oppurtunity presented itself, it was literally handed to you on a silver platter.. and instead you people all treated me like shit and gambled on me failing with my project. I ask you, where has your common sense gone? I hate what this will do to everyone around me, this is never what I wanted. PS: I know Microsoft's PR teams will say a lot of awful shit about me, completely lacking of context, because that is just the type of people they are. If you want to know who I really am, I suggest you talk to my former teamlead and my former manager who Microsoft retaliated against for defending me. I'm done with this entire shit industry. Everything that happened, you should think whose fault this really is. Maybe instead of morrons sitting on golden thrones build with big tech money, pointing fingers at everyone whose skills and drive hurt their fragile little egoes.. maybe start using your brains, because you people keep losing and keep making things worse.

Note: I am antifa, leader of the cipherpunk, fuck the FBI department. 

#### To run from folder "QS_variant" (Standard SIQS with our number theory as backend):</br></br>
To build: python3 setup.py build_ext --inplace</br>
To run: python3 run_qs.py -keysize 200 -base 2000 -debug 1 -lin_size 100_000 -quad_size 1</br></br>

See below for an improved way of performing what this PoC does.. I'll delete this Proof of Concept once the PoC for Improved_QS_Variant matures a little more<br><br>
#### To run from folder "Improved_QS_Variant" (Implements more of my number theory to also take advantage of quadratic coefficients):</br></br>

To build: python3 setup.py build_ext --inplace</br>
To run: python3 run_qs.py -keysize 140  -base 4000 -debug 1 -lin_size 1_000_000 -quad_size 1</br></br>

Just use the command above on 140-bit semi primes until I optimize the rest of the PoC.

Today I messed around a bit with the settings and found an ideal combination. So the above command uses a factor base of 4000 primes, yet we consistently finish early with less then 1000 smooths (default SIQS would need on everage as many smooths as the factor base... so this is a big improvement). 

We achieve this with the variable g_small_prime_limit=5000. This will mark the sieve interval with odd and even exponents for primes less then 5000, and only even exponents for those greater then 5000. In addition, if a smooth is found, then we try to find more smooths that contain the largest prime factors. This is a very important step, because this greatly contributes to lowering the required amonut of primes. The reason this works is because we try to limit the amount of large primes with odd exponents... and if we do find large primes with odd exponents, we use find_similar() to find more smooths specifically with those primes. 

Now I am fairly confident I finally have something worth publishing. I need to optimize the hell out of everything now. As finding smooths in find_similar() is vital to requiring far fewer smooths, I must greatly optimize all of that. One thing I wonder is if I can simply shift results for one quadratic coefficient to other quadratic coefficients without recalculating congruences and all that. So I can construct those sieve intervals very quickly. There is definitely a lot worth exploring optimization wise. I think once I can factor 80 digits faster then the PoC in QS_Variant and with fewer smooths.. I'll begin finalizing the paper and prepare to publish my results. Once that then is out of the way, I'll keep exploring how far I can push this approach, and hopefully my future will begin to look less bleak... because it has been looking for bleak for nearly 3 years now..

Update: My goal for tomorrow is optimizing find_similar() .. we NEED to find those similar smooths. So that part has to go fast. When building the sieve interval, I'm sure I just need to calculate those distances once for each prime... and then I should be able to extrapolate what those distances will be for other quadratic coefficients. Cant be so hard, it's just residue math. There has to be some type of shortcut there. I solve this... and I should be getting the performance I want after that and be able to move forward with formally publishing my findings (well, just making a lot of noise about it, bc I cant publish things since I'm not an academic).

Update: So fucking broke. I guess getting into ultra marathons and trail running isn't helping. You would think running is one of the cheapest hobbies.. but damn... running 60k-100k a week wears out your gear so fast and its fucking expensive. Bought a pair of hoka speedgoats and cliftons this week (so I can do long runs on trail or road)... I ended up with an overuse injury when I was running 60k ultra marathons every sunday on the road... so trying some shoes with bigger stack heigth now to see if that helps I guess. I want to try and run 100k this summer. Running is the only thing keeping me sane honestly. But it's going to be hard to keep up with the gear and nutrition I need without income. Anyway.. today I will improve root calculations for different quadratic coefficients. This will improve both the initial phase of the PoC and the stuff in find_similar(). I did some thinking already about it in my head, and I think I have a way to do it. Especially now that I understand that relation between roots and coefficients.

Update: EUREKA! It can be done very quickly. All we need to do is precalculate the inverse of each quadratic coefficient mod p and run tonelli on that. That gives us a value we can multiple roots with in find_similar. As long as we have a start root at quadratic coefficient = 1, then we can just get all the other roots by multiplying with this precalculated value. And in theory we can also reduce the size of the huge datastructure created in create_hashmap (what I call the iN hashmap.. although its an array.. it used to be a hashmap) ... because we really just only need those roots at quadratic coefficient = 1 and everything else can be derived from that. Big big big speed boost incoming.... it's game over guys. Good luck, because you're going to need it. I might just go for a run early today.. I'll probably upload this tomorrow.
